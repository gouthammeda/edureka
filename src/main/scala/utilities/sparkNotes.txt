Spark Architecture:
spark context object in driver program co-ordinates all distributed and resource allocation like,
how many executors to be launched with memory and number of cores
executor is a process that is launched for spark application on worker node.

cluster manager provide executors with jvm process with logic.
spark context object sends application to cluster and executes task in each executor.

spark deployment modes:
Amazon EC2, stand alone mode, mesos,yarn

spark deployment modes on YARN:
in cluster mode spark driver runs in application master on cluster host.
single process in a YARN container is responsible for both driving the application and requesting resources from YARN.

In client mode, driver runs on the host where job is submitted.
application master is merely present to request executor containers from YARN.
client communicates with containers to schedule work after they start.
spark shell is known as driver program running in the same machine where job is submitted ,so it is client mode
In the cluster mode we just take input from one HDFS location and submit output to another HDFS location
but in client mode we execute sample programs in shell and see the results.
when we create a spark context object then instance is automatically created, so it becomes executable

dynamically loading spark properties:
val sc = new SparkContext(new SparkConf())
spark - submit -- name
"My app" -- master yarn -- conf spark.eventLog.enabled = false
-- conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" App
.jar

rdd try to solve the problems of distributive iterative computing by enabling fault-tolerant, distributed
in-memory computations
since rdd are created over set of transformations, it logs those transformations, rather than actual data
if we lose some partition of RDD, we can replay transformation on that partition in lineage to achieve
same computation, rather than doing data replication across multiple nodes.

RDD:Resilient Distributed Datasets.
rdd is a distributed memory abstraction which lets programmers perform in-memory computations on large cluster in
a fault-tolerant manner.

there are several operations performed on rdd's:
1.functions, transformations, actions
In-memory computation, Fault Tolerance, Persistence, lazy evaluation, coarse grained operation, persistence, partitioning
and immutability
coarse grained operations --> applies on each and every element in dataset through map,flatmap and group by operations.

narrow vs wide transform:
Number of task depends on number of partition
Narrow Transformation - No Shuffling is required,Example - Map transformation is narrow Transformation
wide Transformation - shuffling is required, Example - reduceByKey is wide transformation
for each wide transformation a new stage is created

coalesce and repartition:
in coalesce there is minimum movement of data takes place across a partition but would become bottleneck for
further processing for partition that has more records when compared with partition of lesser records.
second and third partition are added of total 3 partitions and first remains untouched in coalesce
repartition is where lot of shuffling takes place but data is equally distributed among partition further processing
on those records will be faster.repartition is expensive processes as compared with coalesce as there is a lot of
shuffling is happening.in repartition the order is not maintained in partitions.1 partition may be present on
totally different server as well so performance wise repartition is costly as data might be transferred through a network.
coalesce only used for decreasing the partitions with repartition we can increase the partitions.
if there are four partitions and coalesce of 2 is given then records of 2 partitions will be merged using coalesce.

persistence level:
mem_only: when we have 12 gb of data but 10 gb of ram in mem only all 10 gb is calculated and stored in memory
and remaining 2gb is calculated when we hit collect on it
mem_disk: when we have 12 gb of data all 12 gb is taken and calculated and 10 gb is stored in mem and remaining 2 gb in disk.
serialization: converting textual data to byte stream(o's and 1's)
deserialization: converting byte stream to textual data
saving in serialized format is used if there is a lot of shuffling needs to happen on data we prefer to store
it as memory_only_ser as opposed to adding an extra step of serialization in mem_only
but if shuffling is not required then we can store it as mem_only as deserialization is not required and cpu is not highly used.
